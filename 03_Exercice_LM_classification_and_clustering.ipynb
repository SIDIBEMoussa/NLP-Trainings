{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoR46tvNf6rJ84VuWqKWwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDIBEMoussa/NLP-Trainings/blob/main/03_Exercice_LM_classification_and_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b-mUm2hQtoX",
        "outputId": "b9b6b638-85a7-44da-d9c1-455cb6dc0e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, os"
      ],
      "metadata": {
        "id": "TnecPRtIWSHh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/datasets/lda_sports_politics_docs\"\n",
        "\n",
        "file_directory = pathlib.Path(path).exists()\n",
        "\n",
        "if not file_directory:\n",
        "\n",
        "    !git clone https://github.com/selva86/datasets.git\n",
        "\n",
        "\n",
        "\n",
        "text_files = os.listdir(path)"
      ],
      "metadata": {
        "id": "bwSAiHVcXKtV"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJt9xVauXgw-",
        "outputId": "00928ae1-66ff-4fb6-c1c6-b5fa53458ec0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shinzo abe.txt',\n",
              " 'pizza.txt',\n",
              " 'lee quan yew.txt',\n",
              " 'queen elizabeth.txt',\n",
              " 'cricket.txt',\n",
              " 'idli.txt',\n",
              " 'barack obama.txt',\n",
              " 'baseball.txt',\n",
              " 'badminton.txt',\n",
              " 'noodles.txt',\n",
              " 'tipu sultan.txt',\n",
              " 'table tennis.txt',\n",
              " 'pasta.txt',\n",
              " 'narendra modi.txt',\n",
              " 'dosa.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles=[]\n",
        "\n",
        "for filename in text_files:\n",
        "\n",
        "    article = pathlib.Path(path+'/'+filename).read_text()\n",
        "\n",
        "    articles.append(article)"
      ],
      "metadata": {
        "id": "oU_znckrYutX"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "gv_gU5CKa579",
        "outputId": "f914e3c7-11a7-4d33-c982-2d881b29d829"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Shinzō Abe (Abe Shinzō, IPA: born 21 September 1954) is the current Prime Minister of Japan, re-elected to the position in December 2012. Abe is also the President of the Liberal Democratic Party (LDP).\\nAbe served for a year as Prime Minister, from 2006 to 2007. Hailing from a politically prominent family, at age 52, Abe became Japan's youngest post-war prime minister, and the first to be born after World War II, when he was elected by a special session of the National Diet in September 2006. Abe resigned on 12 September 2007, for health reasons. Abe was replaced by Yasuo Fukuda, beginning a string of five Prime Ministers, none of whom retained office for more than sixteen months, before Abe staged a political comeback.\\nOn 26 September 2012, Abe defeated former Minister of Defense Shigeru Ishiba, in a run-off vote, to win the LDP Presidential Election. Following the LDP's landslide victory in the 2012 general election, Abe became the Prime Minister again. Abe is the first former Prime Minister to return to the office since Shigeru Yoshida in 1948. Abe was re-elected at the 2014 general election, retaining his two-thirds majority with coalition partner Komeito.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint # for printing objects nicely\n",
        "\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# from gensim.parsing.preprocessing import STOPWORDS \n",
        "# Instead of the gensim english stopwords, we use nltk's German stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from random import choice\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "german_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(text)\n",
        "\n",
        "def preprocess(text):\n",
        "  result = [lemmatize_stemming(token) \n",
        "            for token in simple_preprocess(text)  \n",
        "            if token not in german_stop_words and len(token) > 3]\n",
        "  return result"
      ],
      "metadata": {
        "id": "fMJuWnswbPZP"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('original document: ')\n",
        "article = choice(articles)\n",
        "print(article)\n",
        "\n",
        "# This time, we don't care about punctuations as tokens (Can you think why?):\n",
        "print('original document, broken into words: ')\n",
        "words = [word for word in article.split(' ')]\n",
        "print(words)\n",
        "print(\"Vocabulary size of the original article:\", len(set(words)))\n",
        "\n",
        "# now let's see what happens when we pass the article into our preprocessing \n",
        "# method:\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "preprocessed_article = preprocess(article)\n",
        "print(preprocessed_article)\n",
        "print(\"Vocabulary size after preprocessing:\", len(set(preprocessed_article)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHzvndkJl6Qt",
        "outputId": "8a2a80dc-8397-4c61-e27d-c7484b754237"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original document: \n",
            "Elizabeth II (Elizabeth Alexandra Mary; born 21 April 1926) is, and has been since her accession in 1952, Queen of theUnited Kingdom, Canada, Australia, and New Zealand, and Head of the Commonwealth. She is also Queen of 12 countries that have become independent since her accession: Jamaica, Barbados, the Bahamas, Grenada, Papua New Guinea,Solomon Islands, Tuvalu, Saint Lucia, Saint Vincent and the Grenadines, Belize, Antigua and Barbuda, and Saint Kitts and Nevis.\n",
            "Elizabeth was born in London to the Duke and Duchess of York, later King George VI and Queen Elizabeth, and was the elder of their two daughters. She was educated privately at home. Her father acceded to the throne on the abdication of his brother Edward VIII in 1936, from which time she was the heir presumptive. She began to undertake public duties during theSecond World War, serving in the Auxiliary Territorial Service. In 1947, she married Philip, Duke of Edinburgh, with whom she has four children: Charles, Anne, Andrew, and Edward.\n",
            "\n",
            "original document, broken into words: \n",
            "['Elizabeth', 'II', '(Elizabeth', 'Alexandra', 'Mary;', 'born', '21', 'April', '1926)', 'is,', 'and', 'has', 'been', 'since', 'her', 'accession', 'in', '1952,', 'Queen', 'of', 'theUnited', 'Kingdom,', 'Canada,', 'Australia,', 'and', 'New', 'Zealand,', 'and', 'Head', 'of', 'the', 'Commonwealth.', 'She', 'is', 'also', 'Queen', 'of', '12', 'countries', 'that', 'have', 'become', 'independent', 'since', 'her', 'accession:', 'Jamaica,', 'Barbados,', 'the', 'Bahamas,', 'Grenada,', 'Papua', 'New', 'Guinea,Solomon', 'Islands,', 'Tuvalu,', 'Saint', 'Lucia,', 'Saint', 'Vincent', 'and', 'the', 'Grenadines,', 'Belize,', 'Antigua', 'and', 'Barbuda,', 'and', 'Saint', 'Kitts', 'and', 'Nevis.\\nElizabeth', 'was', 'born', 'in', 'London', 'to', 'the', 'Duke', 'and', 'Duchess', 'of', 'York,', 'later', 'King', 'George', 'VI', 'and', 'Queen', 'Elizabeth,', 'and', 'was', 'the', 'elder', 'of', 'their', 'two', 'daughters.', 'She', 'was', 'educated', 'privately', 'at', 'home.', 'Her', 'father', 'acceded', 'to', 'the', 'throne', 'on', 'the', 'abdication', 'of', 'his', 'brother', 'Edward', 'VIII', 'in', '1936,', 'from', 'which', 'time', 'she', 'was', 'the', 'heir', 'presumptive.', 'She', 'began', 'to', 'undertake', 'public', 'duties', 'during', 'theSecond', 'World', 'War,', 'serving', 'in', 'the', 'Auxiliary', 'Territorial', 'Service.', 'In', '1947,', 'she', 'married', 'Philip,', 'Duke', 'of', 'Edinburgh,', 'with', 'whom', 'she', 'has', 'four', 'children:', 'Charles,', 'Anne,', 'Andrew,', 'and', 'Edward.\\n']\n",
            "Vocabulary size of the original article: 117\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['elizabeth', 'elizabeth', 'alexandra', 'mari', 'born', 'april', 'sinc', 'access', 'queen', 'theunit', 'kingdom', 'canada', 'australia', 'zealand', 'head', 'commonwealth', 'also', 'queen', 'countri', 'becom', 'independ', 'sinc', 'access', 'jamaica', 'barbado', 'bahama', 'grenada', 'papua', 'guinea', 'solomon', 'island', 'tuvalu', 'saint', 'lucia', 'saint', 'vincent', 'grenadin', 'beliz', 'antigua', 'barbuda', 'saint', 'kitt', 'nevi', 'elizabeth', 'born', 'london', 'duke', 'duchess', 'york', 'later', 'king', 'georg', 'queen', 'elizabeth', 'elder', 'daughter', 'educ', 'privat', 'home', 'father', 'acced', 'throne', 'abdic', 'brother', 'edward', 'viii', 'time', 'heir', 'presumpt', 'began', 'undertak', 'public', 'duti', 'thesecond', 'world', 'serv', 'auxiliari', 'territori', 'servic', 'marri', 'philip', 'duke', 'edinburgh', 'four', 'children', 'charl', 'ann', 'andrew', 'edward']\n",
            "Vocabulary size after preprocessing: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs = list(map(preprocess, articles))\n",
        "processed_docs[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CscbY6DmTnq",
        "outputId": "33a097d0-a9eb-4035-8e1d-b7c5906f5915"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['shinzō',\n",
              "  'shinzō',\n",
              "  'born',\n",
              "  'septemb',\n",
              "  'current',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'japan',\n",
              "  'elect',\n",
              "  'posit',\n",
              "  'decemb',\n",
              "  'also',\n",
              "  'presid',\n",
              "  'liber',\n",
              "  'democrat',\n",
              "  'parti',\n",
              "  'serv',\n",
              "  'year',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'hail',\n",
              "  'polit',\n",
              "  'promin',\n",
              "  'famili',\n",
              "  'becam',\n",
              "  'japan',\n",
              "  'youngest',\n",
              "  'post',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'first',\n",
              "  'born',\n",
              "  'world',\n",
              "  'elect',\n",
              "  'special',\n",
              "  'session',\n",
              "  'nation',\n",
              "  'diet',\n",
              "  'septemb',\n",
              "  'resign',\n",
              "  'septemb',\n",
              "  'health',\n",
              "  'reason',\n",
              "  'replac',\n",
              "  'yasuo',\n",
              "  'fukuda',\n",
              "  'begin',\n",
              "  'string',\n",
              "  'five',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'none',\n",
              "  'retain',\n",
              "  'offic',\n",
              "  'sixteen',\n",
              "  'month',\n",
              "  'stage',\n",
              "  'polit',\n",
              "  'comeback',\n",
              "  'septemb',\n",
              "  'defeat',\n",
              "  'former',\n",
              "  'minist',\n",
              "  'defens',\n",
              "  'shigeru',\n",
              "  'ishiba',\n",
              "  'vote',\n",
              "  'presidenti',\n",
              "  'elect',\n",
              "  'follow',\n",
              "  'landslid',\n",
              "  'victori',\n",
              "  'general',\n",
              "  'elect',\n",
              "  'becam',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'first',\n",
              "  'former',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'return',\n",
              "  'offic',\n",
              "  'sinc',\n",
              "  'shigeru',\n",
              "  'yoshida',\n",
              "  'elect',\n",
              "  'general',\n",
              "  'elect',\n",
              "  'retain',\n",
              "  'third',\n",
              "  'major',\n",
              "  'coalit',\n",
              "  'partner',\n",
              "  'komeito'],\n",
              " ['pizza',\n",
              "  'flatbread',\n",
              "  'general',\n",
              "  'top',\n",
              "  'tomato',\n",
              "  'sauc',\n",
              "  'chees',\n",
              "  'bake',\n",
              "  'oven',\n",
              "  'common',\n",
              "  'top',\n",
              "  'select',\n",
              "  'meat',\n",
              "  'veget',\n",
              "  'condiment',\n",
              "  'term',\n",
              "  'first',\n",
              "  'record',\n",
              "  'centuri',\n",
              "  'latin',\n",
              "  'manuscript',\n",
              "  'gaeta',\n",
              "  'central',\n",
              "  'itali',\n",
              "  'modern',\n",
              "  'pizza',\n",
              "  'invent',\n",
              "  'napl',\n",
              "  'itali',\n",
              "  'dish',\n",
              "  'variant',\n",
              "  'sinc',\n",
              "  'becom',\n",
              "  'popular',\n",
              "  'mani',\n",
              "  'area',\n",
              "  'world',\n",
              "  'upon',\n",
              "  'itali',\n",
              "  'request',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'safeguard',\n",
              "  'european',\n",
              "  'union',\n",
              "  'tradit',\n",
              "  'special',\n",
              "  'guarante',\n",
              "  'dish',\n",
              "  'associazion',\n",
              "  'verac',\n",
              "  'pizza',\n",
              "  'napoletana',\n",
              "  'true',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'associ',\n",
              "  'profit',\n",
              "  'organis',\n",
              "  'found',\n",
              "  'headquart',\n",
              "  'napl',\n",
              "  'promot',\n",
              "  'protect',\n",
              "  'true',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'pizza',\n",
              "  'sold',\n",
              "  'fresh',\n",
              "  'frozen',\n",
              "  'either',\n",
              "  'whole',\n",
              "  'portion',\n",
              "  'common',\n",
              "  'fast',\n",
              "  'food',\n",
              "  'item',\n",
              "  'europ',\n",
              "  'north',\n",
              "  'america',\n",
              "  'various',\n",
              "  'type',\n",
              "  'oven',\n",
              "  'use',\n",
              "  'cook',\n",
              "  'mani',\n",
              "  'varieti',\n",
              "  'exist',\n",
              "  'sever',\n",
              "  'similar',\n",
              "  'dish',\n",
              "  'prepar',\n",
              "  'ingredi',\n",
              "  'common',\n",
              "  'use',\n",
              "  'pizza',\n",
              "  'prepar',\n",
              "  'calzon',\n",
              "  'stromboli']]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(processed_docs)"
      ],
      "metadata": {
        "id": "bdoCd4MeoUTT"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (k, v) in enumerate(dictionary.iteritems()):\n",
        "    print(k, v)\n",
        "    if idx >= 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-zj_g3iop0-",
        "outputId": "115c764d-0b4b-4d95-9d50-c2af15b66293"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 also\n",
            "1 becam\n",
            "2 begin\n",
            "3 born\n",
            "4 coalit\n",
            "5 comeback\n",
            "6 current\n",
            "7 decemb\n",
            "8 defeat\n",
            "9 defens\n",
            "10 democrat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Hyperparameter tuning"
      ],
      "metadata": {
        "id": "TtxfwvLko5z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model hyper parameters:\n",
        "\n",
        "## These are the dictionary preparation parameters:\n",
        "filter_tokens_if_container_documents_are_less_than = 1\n",
        "filter_tokens_if_appeared_percentage_more_than = 0.5\n",
        "keep_the_first_n_tokens=100000\n",
        "\n",
        "## and the LDA Parameters: \n",
        "num_of_topics = 10"
      ],
      "metadata": {
        "id": "q_9o3iwgovvs"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(\n",
        "        no_below=filter_tokens_if_container_documents_are_less_than, \n",
        "        no_above=filter_tokens_if_appeared_percentage_more_than, \n",
        "        keep_n=keep_the_first_n_tokens)"
      ],
      "metadata": {
        "id": "pc_2C0A_P49e"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.keys()==[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD0l0OHOP-YV",
        "outputId": "8a4f887d-88c3-454f-c76a-4ab6744acc6f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,50):\n",
        "\n",
        "    filter_tokens_if_container_documents_are_less_than = 10\n",
        "    filter_tokens_if_appeared_percentage_more_than = 0.5\n",
        "    keep_the_first_n_tokens=10*i\n",
        "\n",
        "\n",
        "    dictionary.filter_extremes(\n",
        "        no_below=filter_tokens_if_container_documents_are_less_than, \n",
        "        no_above=filter_tokens_if_appeared_percentage_more_than, \n",
        "        keep_n=keep_the_first_n_tokens)\n",
        "    \n",
        "    if dictionary.keys()!=[]:\n",
        "\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "Dl_KUjqVpIfA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GpuBaq2Hkm5",
        "outputId": "fd0beff6-b96a-4564-ba0f-7a8d013a02f0"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "bow_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSBwUHL4pOwV",
        "outputId": "5a5a1bfe-4205-4ecc-86f9-2cb5c8113dec"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly choose an article from the corpus:\n",
        "sample_bow_doc = choice(bow_corpus)\n",
        "\n",
        "print('The processed bag-of-word document is just pairs of (word_id, # of occurnces) and looks like this:')\n",
        "print(sample_bow_doc, '\\n\\n')\n",
        "\n",
        "print ('We peek in the dictionary: for each word_id, we get its assigned word:')\n",
        "for word_id, word_freq in sample_bow_doc:\n",
        "  real_word = dictionary[word_id]\n",
        "  print(f'Word {word_id} (\"{real_word}\") appears {word_freq} time.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X14slQgapdFj",
        "outputId": "abe363b6-48d3-4816-a61e-73d69effa680"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The processed bag-of-word document is just pairs of (word_id, # of occurnces) and looks like this:\n",
            "[] \n",
            "\n",
            "\n",
            "We peek in the dictionary: for each word_id, we get its assigned word:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA model using Bag-of-words"
      ],
      "metadata": {
        "id": "_ahfDQuTq-AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = models.LdaMulticore(bow_corpus, \n",
        "                                num_topics=num_of_topics, \n",
        "                                id2word=dictionary, \n",
        "                                passes=5, \n",
        "                                workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "GpGCUPDIpxvB",
        "outputId": "aa5cf177-39e8-4f78-9bec-2d68f1830aad"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-7931bc155495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lda_model = models.LdaMulticore(bow_corpus, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_of_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 workers=2)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auto-tuning alpha not implemented in multicore LDA; use plain LdaModel.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         super(LdaMulticore, self).__init__(\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(num_of_topics):\n",
        "    print(f'Topic: {idx} \\t Words: {topic}')"
      ],
      "metadata": {
        "id": "pUFKZu13rC-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF / IDF"
      ],
      "metadata": {
        "id": "u7LgcbhKrQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a tfidf from our corpus\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "\n",
        "# apply it on our corpus \n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "pprint(tfidf_corpus[0][:10])"
      ],
      "metadata": {
        "id": "wbff7C1KrJix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the new tfidf corpus is just our corpus - but transformed. It has the same size of documents:\n",
        "assert len(bow_corpus) == len(tfidf_corpus)"
      ],
      "metadata": {
        "id": "e4oCnXR2rJen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply LDA on the tfidf corpus, with the same amount of topics.\n",
        "\n",
        "You can play with the # of passes, if the model doesn't converge properly"
      ],
      "metadata": {
        "id": "DxizY1d9rcqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_tfidf = models.LdaMulticore(tfidf_corpus, \n",
        "                                      num_topics=num_of_topics, \n",
        "                                      id2word=dictionary, \n",
        "                                      passes=5, \n",
        "                                      workers=4)"
      ],
      "metadata": {
        "id": "kaKEXtUarJXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model_tfidf.print_topics(num_of_topics):\n",
        "    print(f'Topic: {idx} \\t Word: {topic}')"
      ],
      "metadata": {
        "id": "dZf1dpQ5rgdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Now that we have a topic-modeler, let's use it on one of the articles."
      ],
      "metadata": {
        "id": "rRmuhjMvrrhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly pick an article:\n",
        "test_doc = choice(range(len(processed_docs)))\n",
        "processed_docs[test_doc][:50]"
      ],
      "metadata": {
        "id": "U5vc4WDarknL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the original BOW model:"
      ],
      "metadata": {
        "id": "dlqMzBPdr1yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[test_doc]], key=lambda tup: -1*tup[1]):\n",
        "    print(f\"Topic match score: {score} \\nTopic: {lda_model.print_topic(index, num_of_topics)}\")\n"
      ],
      "metadata": {
        "id": "EEC7IrEXrw9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with the TF/IDF model:"
      ],
      "metadata": {
        "id": "apxeLH5JsBRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, score in sorted(lda_model_tfidf[bow_corpus[test_doc]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Topic match score: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, num_of_topics)))"
      ],
      "metadata": {
        "id": "tCCzFiLVr8QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Perplexity: ', lda_model.log_perplexity(bow_corpus)) \n",
        "print('Perplexity TFIDF: ', lda_model_tfidf.log_perplexity(bow_corpus)) "
      ],
      "metadata": {
        "id": "Eyz4sodNsKln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - inference\n",
        "\n",
        "Now try it on a new document!\n",
        "\n",
        "Go to a news website, such as [orf.at](https://orf.at/) and copy an article of your choice here:"
      ],
      "metadata": {
        "id": "WlrqLTEFsTDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_document = \"\"\"Sobotka sprach in einer um eineinhalb Stunden verspäteten Pressekonferenz am Abend von einer „schwierigen“, aber „sachlichen und pointierten“ Diskussion im Hauptausschuss. Er meinte, dass von Gesundheitsminister Rudolf Anschober (Grüne), die „Notwendigkeit, Verhältnismäßigkeit und Richtigkeit“ der Maßnahmen erläutert worden seien. Zudem betonte Sobotka die Wichtigkeit der im Covid-Maßnahmengesetz vorgesehenen Mitsprache des Parlaments bei Maßnahmen, bei denen auch die Freiheitsrechte betroffen seien.\n",
        "Der Nationalratspräsident zitierte auch die deutsche Kanzlerin Angela Merkel, die die Maßnahmen im Kampf gegen die Pandemie als „demokratische Zumutung“ bezeichnet hatte. Dem sei nicht nur zuzustimmen, das sei auch zu unterstreichen, sagte er. Die Eindämmung der Pandemie könne aber nur gelingen, wenn sich alle Österreicher einbringen. Der Beschluss alleine reiche nicht, die Maßnahmen müssten auch im Geiste verinnerlicht werden, appellierte Sobotka an die Bevölkerung, die Maßnahmen mitzutragen.\n",
        "Rendi-Wagner kritisiert Regierung\n",
        "Die Opposition hatte schon vor der Sitzung – und dann auch noch einmal danach – ihre Kritik an der Regierung bekräftigt. „Die Bundesregierung hat die Kontrolle über das Infektionsgeschehen verloren und hat die schwierige Situation, vor der wir jetzt stehen, zu verantworten“, kritisierte SPÖ-Klub- und Parteivorsitzende Rendi-Wagner.\n",
        "Sessel auf Tischen in einem geschlossenen Lokal\n",
        "DEBATTEWie gerechtfertigt sind die Maßnahmen?\"\"\"\n",
        "\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "print(\"Simply printing the lda_model output would look like this:\")\n",
        "pprint(lda_model[bow_vector])\n",
        "\n",
        "print(\"\\n\\nSo let's make it nicer, by printing the topic contents:\")\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
      ],
      "metadata": {
        "id": "jgZV3-Z7sXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "dbF83i4lsena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "bow_lda_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
        "\n",
        "pyLDAvis.display(bow_lda_data)"
      ],
      "metadata": {
        "id": "3DAlOWCwsfBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}