{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDIBEMoussa/NLP-Trainings/blob/main/03_Exercice_LM_classification_and_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b-mUm2hQtoX",
        "outputId": "c4b6060e-d9d3-486c-bccb-8a1d2a887cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from random import seed\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings('ignore')\n",
        "\n",
        "seed(1999)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, os"
      ],
      "metadata": {
        "id": "TnecPRtIWSHh"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/datasets/lda_sports_politics_docs\"\n",
        "\n",
        "file_directory = pathlib.Path(path).exists()\n",
        "\n",
        "if not file_directory:\n",
        "\n",
        "    !git clone https://github.com/selva86/datasets.git\n",
        "\n",
        "\n",
        "\n",
        "text_files = os.listdir(path)"
      ],
      "metadata": {
        "id": "bwSAiHVcXKtV"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJt9xVauXgw-",
        "outputId": "94acc3fe-e625-4455-d3b4-02e8b45b8968"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shinzo abe.txt',\n",
              " 'pizza.txt',\n",
              " 'lee quan yew.txt',\n",
              " 'queen elizabeth.txt',\n",
              " 'cricket.txt',\n",
              " 'idli.txt',\n",
              " 'barack obama.txt',\n",
              " 'baseball.txt',\n",
              " 'badminton.txt',\n",
              " 'noodles.txt',\n",
              " 'tipu sultan.txt',\n",
              " 'table tennis.txt',\n",
              " 'pasta.txt',\n",
              " 'narendra modi.txt',\n",
              " 'dosa.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles=[]\n",
        "\n",
        "for filename in text_files:\n",
        "\n",
        "    article = pathlib.Path(path+'/'+filename).read_text()\n",
        "\n",
        "    articles.append(article)"
      ],
      "metadata": {
        "id": "oU_znckrYutX"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "gv_gU5CKa579",
        "outputId": "747f37d2-3928-4283-9ac0-3d4657098c19"
      },
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Shinzō Abe (Abe Shinzō, IPA: born 21 September 1954) is the current Prime Minister of Japan, re-elected to the position in December 2012. Abe is also the President of the Liberal Democratic Party (LDP).\\nAbe served for a year as Prime Minister, from 2006 to 2007. Hailing from a politically prominent family, at age 52, Abe became Japan's youngest post-war prime minister, and the first to be born after World War II, when he was elected by a special session of the National Diet in September 2006. Abe resigned on 12 September 2007, for health reasons. Abe was replaced by Yasuo Fukuda, beginning a string of five Prime Ministers, none of whom retained office for more than sixteen months, before Abe staged a political comeback.\\nOn 26 September 2012, Abe defeated former Minister of Defense Shigeru Ishiba, in a run-off vote, to win the LDP Presidential Election. Following the LDP's landslide victory in the 2012 general election, Abe became the Prime Minister again. Abe is the first former Prime Minister to return to the office since Shigeru Yoshida in 1948. Abe was re-elected at the 2014 general election, retaining his two-thirds majority with coalition partner Komeito.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint # for printing objects nicely\n",
        "\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# from gensim.parsing.preprocessing import STOPWORDS \n",
        "# Instead of the gensim english stopwords, we use nltk's German stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from random import choice\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "german_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(text)\n",
        "\n",
        "def preprocess(text):\n",
        "  result = [lemmatize_stemming(token) \n",
        "            for token in simple_preprocess(text)  \n",
        "            if token not in german_stop_words and len(token) > 3]\n",
        "  return result"
      ],
      "metadata": {
        "id": "fMJuWnswbPZP"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('original document: ')\n",
        "article = choice(articles)\n",
        "print(article)\n",
        "\n",
        "# This time, we don't care about punctuations as tokens (Can you think why?):\n",
        "print('original document, broken into words: ')\n",
        "words = [word for word in article.split(' ')]\n",
        "print(words)\n",
        "print(\"Vocabulary size of the original article:\", len(set(words)))\n",
        "\n",
        "# now let's see what happens when we pass the article into our preprocessing \n",
        "# method:\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "preprocessed_article = preprocess(article)\n",
        "print(preprocessed_article)\n",
        "print(\"Vocabulary size after preprocessing:\", len(set(preprocessed_article)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHzvndkJl6Qt",
        "outputId": "ceffade2-13db-49a4-e1f3-a53f59916e87"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original document: \n",
            "Pizza is a flatbread generally topped with tomato sauce and cheese and baked in an oven. It is commonly topped with a selection of meats, vegetables and condiments. The term was first recorded in the 10th century, in a Latin manuscript from Gaeta in Central Italy. The modern pizza was invented in Naples, Italy, and the dish and its variants have since become popular in many areas of the world.\n",
            "In 2009, upon Italy's request, Neapolitan pizza was safeguarded in the European Union as a Traditional Speciality Guaranteed dish. The Associazione Verace Pizza Napoletana (the True Neapolitan Pizza Association) is a non-profit organisation founded in 1984 with headquarters in Naples. It promotes and protects the \"true Neapolitan pizza\".\n",
            "Pizza is sold fresh or frozen, either whole or in portions, and is a common fast food item in Europe and North America. Various types of ovens are used to cook them and many varieties exist. Several similar dishes are prepared from ingredients commonly used in pizza preparation, such as calzone and stromboli.\n",
            "\n",
            "original document, broken into words: \n",
            "['Pizza', 'is', 'a', 'flatbread', 'generally', 'topped', 'with', 'tomato', 'sauce', 'and', 'cheese', 'and', 'baked', 'in', 'an', 'oven.', 'It', 'is', 'commonly', 'topped', 'with', 'a', 'selection', 'of', 'meats,', 'vegetables', 'and', 'condiments.', 'The', 'term', 'was', 'first', 'recorded', 'in', 'the', '10th', 'century,', 'in', 'a', 'Latin', 'manuscript', 'from', 'Gaeta', 'in', 'Central', 'Italy.', 'The', 'modern', 'pizza', 'was', 'invented', 'in', 'Naples,', 'Italy,', 'and', 'the', 'dish', 'and', 'its', 'variants', 'have', 'since', 'become', 'popular', 'in', 'many', 'areas', 'of', 'the', 'world.\\nIn', '2009,', 'upon', \"Italy's\", 'request,', 'Neapolitan', 'pizza', 'was', 'safeguarded', 'in', 'the', 'European', 'Union', 'as', 'a', 'Traditional', 'Speciality', 'Guaranteed', 'dish.', 'The', 'Associazione', 'Verace', 'Pizza', 'Napoletana', '(the', 'True', 'Neapolitan', 'Pizza', 'Association)', 'is', 'a', 'non-profit', 'organisation', 'founded', 'in', '1984', 'with', 'headquarters', 'in', 'Naples.', 'It', 'promotes', 'and', 'protects', 'the', '\"true', 'Neapolitan', 'pizza\".\\nPizza', 'is', 'sold', 'fresh', 'or', 'frozen,', 'either', 'whole', 'or', 'in', 'portions,', 'and', 'is', 'a', 'common', 'fast', 'food', 'item', 'in', 'Europe', 'and', 'North', 'America.', 'Various', 'types', 'of', 'ovens', 'are', 'used', 'to', 'cook', 'them', 'and', 'many', 'varieties', 'exist.', 'Several', 'similar', 'dishes', 'are', 'prepared', 'from', 'ingredients', 'commonly', 'used', 'in', 'pizza', 'preparation,', 'such', 'as', 'calzone', 'and', 'stromboli.\\n']\n",
            "Vocabulary size of the original article: 113\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['pizza', 'flatbread', 'general', 'top', 'tomato', 'sauc', 'chees', 'bake', 'oven', 'common', 'top', 'select', 'meat', 'veget', 'condiment', 'term', 'first', 'record', 'centuri', 'latin', 'manuscript', 'gaeta', 'central', 'itali', 'modern', 'pizza', 'invent', 'napl', 'itali', 'dish', 'variant', 'sinc', 'becom', 'popular', 'mani', 'area', 'world', 'upon', 'itali', 'request', 'neapolitan', 'pizza', 'safeguard', 'european', 'union', 'tradit', 'special', 'guarante', 'dish', 'associazion', 'verac', 'pizza', 'napoletana', 'true', 'neapolitan', 'pizza', 'associ', 'profit', 'organis', 'found', 'headquart', 'napl', 'promot', 'protect', 'true', 'neapolitan', 'pizza', 'pizza', 'sold', 'fresh', 'frozen', 'either', 'whole', 'portion', 'common', 'fast', 'food', 'item', 'europ', 'north', 'america', 'various', 'type', 'oven', 'use', 'cook', 'mani', 'varieti', 'exist', 'sever', 'similar', 'dish', 'prepar', 'ingredi', 'common', 'use', 'pizza', 'prepar', 'calzon', 'stromboli']\n",
            "Vocabulary size after preprocessing: 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs = list(map(preprocess, articles))\n",
        "processed_docs[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CscbY6DmTnq",
        "outputId": "8ad643c0-f09b-4b65-abe6-ae885c39a212"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['shinzō',\n",
              "  'shinzō',\n",
              "  'born',\n",
              "  'septemb',\n",
              "  'current',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'japan',\n",
              "  'elect',\n",
              "  'posit',\n",
              "  'decemb',\n",
              "  'also',\n",
              "  'presid',\n",
              "  'liber',\n",
              "  'democrat',\n",
              "  'parti',\n",
              "  'serv',\n",
              "  'year',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'hail',\n",
              "  'polit',\n",
              "  'promin',\n",
              "  'famili',\n",
              "  'becam',\n",
              "  'japan',\n",
              "  'youngest',\n",
              "  'post',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'first',\n",
              "  'born',\n",
              "  'world',\n",
              "  'elect',\n",
              "  'special',\n",
              "  'session',\n",
              "  'nation',\n",
              "  'diet',\n",
              "  'septemb',\n",
              "  'resign',\n",
              "  'septemb',\n",
              "  'health',\n",
              "  'reason',\n",
              "  'replac',\n",
              "  'yasuo',\n",
              "  'fukuda',\n",
              "  'begin',\n",
              "  'string',\n",
              "  'five',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'none',\n",
              "  'retain',\n",
              "  'offic',\n",
              "  'sixteen',\n",
              "  'month',\n",
              "  'stage',\n",
              "  'polit',\n",
              "  'comeback',\n",
              "  'septemb',\n",
              "  'defeat',\n",
              "  'former',\n",
              "  'minist',\n",
              "  'defens',\n",
              "  'shigeru',\n",
              "  'ishiba',\n",
              "  'vote',\n",
              "  'presidenti',\n",
              "  'elect',\n",
              "  'follow',\n",
              "  'landslid',\n",
              "  'victori',\n",
              "  'general',\n",
              "  'elect',\n",
              "  'becam',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'first',\n",
              "  'former',\n",
              "  'prime',\n",
              "  'minist',\n",
              "  'return',\n",
              "  'offic',\n",
              "  'sinc',\n",
              "  'shigeru',\n",
              "  'yoshida',\n",
              "  'elect',\n",
              "  'general',\n",
              "  'elect',\n",
              "  'retain',\n",
              "  'third',\n",
              "  'major',\n",
              "  'coalit',\n",
              "  'partner',\n",
              "  'komeito'],\n",
              " ['pizza',\n",
              "  'flatbread',\n",
              "  'general',\n",
              "  'top',\n",
              "  'tomato',\n",
              "  'sauc',\n",
              "  'chees',\n",
              "  'bake',\n",
              "  'oven',\n",
              "  'common',\n",
              "  'top',\n",
              "  'select',\n",
              "  'meat',\n",
              "  'veget',\n",
              "  'condiment',\n",
              "  'term',\n",
              "  'first',\n",
              "  'record',\n",
              "  'centuri',\n",
              "  'latin',\n",
              "  'manuscript',\n",
              "  'gaeta',\n",
              "  'central',\n",
              "  'itali',\n",
              "  'modern',\n",
              "  'pizza',\n",
              "  'invent',\n",
              "  'napl',\n",
              "  'itali',\n",
              "  'dish',\n",
              "  'variant',\n",
              "  'sinc',\n",
              "  'becom',\n",
              "  'popular',\n",
              "  'mani',\n",
              "  'area',\n",
              "  'world',\n",
              "  'upon',\n",
              "  'itali',\n",
              "  'request',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'safeguard',\n",
              "  'european',\n",
              "  'union',\n",
              "  'tradit',\n",
              "  'special',\n",
              "  'guarante',\n",
              "  'dish',\n",
              "  'associazion',\n",
              "  'verac',\n",
              "  'pizza',\n",
              "  'napoletana',\n",
              "  'true',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'associ',\n",
              "  'profit',\n",
              "  'organis',\n",
              "  'found',\n",
              "  'headquart',\n",
              "  'napl',\n",
              "  'promot',\n",
              "  'protect',\n",
              "  'true',\n",
              "  'neapolitan',\n",
              "  'pizza',\n",
              "  'pizza',\n",
              "  'sold',\n",
              "  'fresh',\n",
              "  'frozen',\n",
              "  'either',\n",
              "  'whole',\n",
              "  'portion',\n",
              "  'common',\n",
              "  'fast',\n",
              "  'food',\n",
              "  'item',\n",
              "  'europ',\n",
              "  'north',\n",
              "  'america',\n",
              "  'various',\n",
              "  'type',\n",
              "  'oven',\n",
              "  'use',\n",
              "  'cook',\n",
              "  'mani',\n",
              "  'varieti',\n",
              "  'exist',\n",
              "  'sever',\n",
              "  'similar',\n",
              "  'dish',\n",
              "  'prepar',\n",
              "  'ingredi',\n",
              "  'common',\n",
              "  'use',\n",
              "  'pizza',\n",
              "  'prepar',\n",
              "  'calzon',\n",
              "  'stromboli']]"
            ]
          },
          "metadata": {},
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(processed_docs)"
      ],
      "metadata": {
        "id": "bdoCd4MeoUTT"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (k, v) in enumerate(dictionary.iteritems()):\n",
        "    print(k, v)\n",
        "    if idx >= 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-zj_g3iop0-",
        "outputId": "f55e2c26-f91a-4a6a-be9a-543c6af290a4"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 also\n",
            "1 becam\n",
            "2 begin\n",
            "3 born\n",
            "4 coalit\n",
            "5 comeback\n",
            "6 current\n",
            "7 decemb\n",
            "8 defeat\n",
            "9 defens\n",
            "10 democrat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Hyperparameter tuning"
      ],
      "metadata": {
        "id": "TtxfwvLko5z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model hyper parameters:\n",
        "\n",
        "## These are the dictionary preparation parameters:\n",
        "filter_tokens_if_container_documents_are_less_than = 1\n",
        "filter_tokens_if_appeared_percentage_more_than = 0.5\n",
        "keep_the_first_n_tokens=100000\n",
        "\n",
        "## and the LDA Parameters: \n",
        "num_of_topics = 4"
      ],
      "metadata": {
        "id": "q_9o3iwgovvs"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(\n",
        "        no_below=filter_tokens_if_container_documents_are_less_than, \n",
        "        no_above=filter_tokens_if_appeared_percentage_more_than, \n",
        "        keep_n=keep_the_first_n_tokens)"
      ],
      "metadata": {
        "id": "pc_2C0A_P49e"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimization(processed_docs,min_topic=4, max_topic=10,more_than_number=10,less_than_range=20):\n",
        "\n",
        "    model_results = {#'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'More_than': [],\n",
        "                 'Less_than': [],\n",
        "                 'Perplexity': []\n",
        "                }\n",
        "    keep_the_first_n_tokens = 10000\n",
        "    \n",
        "    dictionary = corpora.Dictionary(processed_docs)\n",
        "\n",
        "    for more_than in np.linspace(0,1,more_than_number):\n",
        "\n",
        "      for less_than in range(1,less_than_range):\n",
        "\n",
        "          dictionary.filter_extremes(\n",
        "          no_below = less_than, \n",
        "          no_above = more_than, \n",
        "          keep_n = keep_the_first_n_tokens)\n",
        "          bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "          \n",
        "          if list(dictionary.keys())!=[]:\n",
        "              print(list(dictionary.keys()))\n",
        "              lda_model = models.LdaMulticore(bow_corpus, \n",
        "                                    num_topics=4, \n",
        "                                    id2word=dictionary,\n",
        "                                    passes=10, \n",
        "                                    workers=2)\n",
        "              \n",
        "              model_results[\"More_than\"].append(more_than)\n",
        "              model_results[\"Less_than\"].append(less_than)\n",
        "              model_results[\"Perplexity\"].append(lda_model.log_perplexity(bow_corpus))\n",
        "    \n",
        "    return model_results"
      ],
      "metadata": {
        "id": "DenJMAVwrPZ4"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results=optimization(processed_docs,min_topic=2, max_topic=10,more_than_number=10,less_than_range=10)"
      ],
      "metadata": {
        "id": "FE_PQ8VJz3em"
      },
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results"
      ],
      "metadata": {
        "id": "1gopoMEp4oss",
        "outputId": "b1c11c82-e346-4800-af4e-d8a116bee6db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Topics': [], 'More_than': [], 'Less_than': [], 'Perplexity': []}"
            ]
          },
          "metadata": {},
          "execution_count": 359
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "#bow_corpus[:1]"
      ],
      "metadata": {
        "id": "iSBwUHL4pOwV"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly choose an article from the corpus:\n",
        "sample_bow_doc = choice(bow_corpus)\n",
        "\n",
        "print('The processed bag-of-word document is just pairs of (word_id, # of occurnces) and looks like this:')\n",
        "print(sample_bow_doc, '\\n\\n')\n",
        "\n",
        "print ('We peek in the dictionary: for each word_id, we get its assigned word:')\n",
        "for word_id, word_freq in sample_bow_doc:\n",
        "  real_word = dictionary[word_id]\n",
        "  print(f'Word {word_id} (\"{real_word}\") appears {word_freq} time.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X14slQgapdFj",
        "outputId": "be66cb48-d7f5-4d7d-9d6c-b780bd7b6062"
      },
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The processed bag-of-word document is just pairs of (word_id, # of occurnces) and looks like this:\n",
            "[(14, 1), (107, 3), (125, 1), (131, 2), (149, 1), (168, 2), (181, 1), (230, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 2), (319, 1), (320, 1), (321, 1), (322, 2), (323, 1), (324, 1), (325, 1), (326, 3), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1)] \n",
            "\n",
            "\n",
            "We peek in the dictionary: for each word_id, we get its assigned word:\n",
            "Word 14 (\"five\") appears 1 time.\n",
            "Word 107 (\"popular\") appears 3 time.\n",
            "Word 125 (\"tradit\") appears 1 time.\n",
            "Word 131 (\"variant\") appears 2 time.\n",
            "Word 149 (\"countri\") appears 1 time.\n",
            "Word 168 (\"known\") appears 2 time.\n",
            "Word 181 (\"peopl\") appears 1 time.\n",
            "Word 230 (\"four\") appears 1 time.\n",
            "Word 311 (\"amongst\") appears 1 time.\n",
            "Word 312 (\"anoth\") appears 1 time.\n",
            "Word 313 (\"batter\") appears 1 time.\n",
            "Word 314 (\"black\") appears 1 time.\n",
            "Word 315 (\"bodi\") appears 1 time.\n",
            "Word 316 (\"break\") appears 1 time.\n",
            "Word 317 (\"breakfast\") appears 1 time.\n",
            "Word 318 (\"cake\") appears 2 time.\n",
            "Word 319 (\"consist\") appears 1 time.\n",
            "Word 320 (\"diamet\") appears 1 time.\n",
            "Word 321 (\"enduri\") appears 1 time.\n",
            "Word 322 (\"ferment\") appears 2 time.\n",
            "Word 323 (\"goan\") appears 1 time.\n",
            "Word 324 (\"household\") appears 1 time.\n",
            "Word 325 (\"husk\") appears 1 time.\n",
            "Word 326 (\"id\") appears 3 time.\n",
            "Word 327 (\"inch\") appears 1 time.\n",
            "Word 328 (\"india\") appears 1 time.\n",
            "Word 329 (\"indian\") appears 1 time.\n",
            "Word 330 (\"konkani\") appears 1 time.\n",
            "Word 331 (\"lanka\") appears 1 time.\n",
            "Word 332 (\"lentil\") appears 1 time.\n",
            "Word 333 (\"like\") appears 1 time.\n",
            "Word 334 (\"made\") appears 1 time.\n",
            "Word 335 (\"metabolis\") appears 1 time.\n",
            "Word 336 (\"neighbour\") appears 1 time.\n",
            "Word 337 (\"odisha\") appears 1 time.\n",
            "Word 338 (\"pitha\") appears 1 time.\n",
            "Word 339 (\"process\") appears 1 time.\n",
            "Word 340 (\"readili\") appears 1 time.\n",
            "Word 341 (\"rice\") appears 1 time.\n",
            "Word 342 (\"sanna\") appears 1 time.\n",
            "Word 343 (\"savouri\") appears 1 time.\n",
            "Word 344 (\"south\") appears 1 time.\n",
            "Word 345 (\"starch\") appears 1 time.\n",
            "Word 346 (\"steam\") appears 1 time.\n",
            "Word 347 (\"throughout\") appears 1 time.\n",
            "Word 348 (\"usual\") appears 1 time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA model using Bag-of-words"
      ],
      "metadata": {
        "id": "_ahfDQuTq-AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = models.LdaMulticore(bow_corpus, \n",
        "                                num_topics=num_of_topics, \n",
        "                                id2word=dictionary, \n",
        "                                passes=10, \n",
        "                                workers=2)"
      ],
      "metadata": {
        "id": "GpGCUPDIpxvB"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(num_of_topics):\n",
        "    print(f'Topic: {idx} \\t Words: {topic}')"
      ],
      "metadata": {
        "id": "pUFKZu13rC-V",
        "outputId": "45786aa6-2c44-4e78-b274-453b3bee962e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \t Words: 0.022*\"minist\" + 0.018*\"noodl\" + 0.016*\"prime\" + 0.016*\"elect\" + 0.010*\"septemb\" + 0.008*\"serv\" + 0.008*\"general\" + 0.008*\"offic\" + 0.008*\"sinc\" + 0.008*\"gujarat\"\n",
            "Topic: 1 \t Words: 0.023*\"pizza\" + 0.012*\"ball\" + 0.012*\"tabl\" + 0.009*\"dish\" + 0.009*\"common\" + 0.009*\"use\" + 0.009*\"player\" + 0.009*\"itali\" + 0.009*\"neapolitan\" + 0.006*\"mani\"\n",
            "Topic: 2 \t Words: 0.015*\"tipu\" + 0.015*\"dosa\" + 0.012*\"mysor\" + 0.007*\"team\" + 0.007*\"father\" + 0.006*\"presid\" + 0.006*\"democrat\" + 0.006*\"unit\" + 0.006*\"primari\" + 0.006*\"state\"\n",
            "Topic: 3 \t Words: 0.024*\"pasta\" + 0.016*\"shape\" + 0.010*\"form\" + 0.008*\"known\" + 0.008*\"also\" + 0.008*\"play\" + 0.008*\"fresh\" + 0.008*\"elizabeth\" + 0.006*\"name\" + 0.006*\"dri\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF / IDF"
      ],
      "metadata": {
        "id": "u7LgcbhKrQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a tfidf from our corpus\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "\n",
        "# apply it on our corpus \n",
        "tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "pprint(tfidf_corpus[0][:10])"
      ],
      "metadata": {
        "id": "wbff7C1KrJix",
        "outputId": "f74e22c5-2e42-4d6b-dc95-ff66af2f1621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.03278411436243488),\n",
            " (1, 0.09458263214900205),\n",
            " (2, 0.09689176633988147),\n",
            " (3, 0.07861485368951274),\n",
            " (4, 0.09689176633988147),\n",
            " (5, 0.09689176633988147),\n",
            " (6, 0.0575843394951251),\n",
            " (7, 0.07209154120719125),\n",
            " (8, 0.07209154120719125),\n",
            " (9, 0.09689176633988147)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the new tfidf corpus is just our corpus - but transformed. It has the same size of documents:\n",
        "assert len(bow_corpus) == len(tfidf_corpus)"
      ],
      "metadata": {
        "id": "e4oCnXR2rJen"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply LDA on the tfidf corpus, with the same amount of topics.\n",
        "\n",
        "You can play with the # of passes, if the model doesn't converge properly"
      ],
      "metadata": {
        "id": "DxizY1d9rcqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_tfidf = models.LdaMulticore(tfidf_corpus, \n",
        "                                      num_topics=num_of_topics, \n",
        "                                      id2word=dictionary, \n",
        "                                      passes=10, \n",
        "                                      workers=4)"
      ],
      "metadata": {
        "id": "kaKEXtUarJXd"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model_tfidf.print_topics(num_of_topics):\n",
        "    print(f'Topic: {idx} \\t Word: {topic}')"
      ],
      "metadata": {
        "id": "dZf1dpQ5rgdj",
        "outputId": "e2033ba9-298e-4f2f-cad4-1b5f0f9b7d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \t Word: 0.004*\"noodl\" + 0.003*\"team\" + 0.003*\"racquet\" + 0.003*\"play\" + 0.003*\"elizabeth\" + 0.003*\"side\" + 0.003*\"queen\" + 0.003*\"saint\" + 0.003*\"badminton\" + 0.003*\"court\"\n",
            "Topic: 1 \t Word: 0.003*\"gujarat\" + 0.003*\"modi\" + 0.003*\"base\" + 0.003*\"plate\" + 0.003*\"improv\" + 0.003*\"around\" + 0.003*\"home\" + 0.003*\"bat\" + 0.003*\"run\" + 0.002*\"minist\"\n",
            "Topic: 2 \t Word: 0.004*\"dosa\" + 0.004*\"minist\" + 0.003*\"prime\" + 0.003*\"elect\" + 0.003*\"popular\" + 0.003*\"id\" + 0.002*\"septemb\" + 0.002*\"obama\" + 0.002*\"primari\" + 0.002*\"unit\"\n",
            "Topic: 3 \t Word: 0.004*\"pasta\" + 0.004*\"pizza\" + 0.004*\"tipu\" + 0.003*\"tabl\" + 0.003*\"mysor\" + 0.003*\"shape\" + 0.002*\"ball\" + 0.002*\"neapolitan\" + 0.002*\"bounc\" + 0.002*\"fresh\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Now that we have a topic-modeler, let's use it on one of the articles."
      ],
      "metadata": {
        "id": "rRmuhjMvrrhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly pick an article:\n",
        "test_doc = choice(range(len(processed_docs)))\n",
        "processed_docs[test_doc][:10]"
      ],
      "metadata": {
        "id": "U5vc4WDarknL",
        "outputId": "cea6f6ee-9035-4b8f-eda4-80eafe4ae376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kuan',\n",
              " 'gcmg',\n",
              " 'spmj',\n",
              " 'born',\n",
              " 'harri',\n",
              " 'kuan',\n",
              " 'septemb',\n",
              " 'march',\n",
              " 'inform',\n",
              " 'known']"
            ]
          },
          "metadata": {},
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the original BOW model:"
      ],
      "metadata": {
        "id": "dlqMzBPdr1yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[test_doc]], key=lambda tup: -1*tup[1]):\n",
        "    print(f\"Topic match score: {score} \\nTopic: {lda_model.print_topic(index, num_of_topics)}\")\n"
      ],
      "metadata": {
        "id": "EEC7IrEXrw9Q",
        "outputId": "536078c2-7b03-47dd-b0c2-057b1b6d8347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic match score: 0.9920631647109985 \n",
            "Topic: 0.015*\"tipu\" + 0.015*\"dosa\" + 0.012*\"mysor\" + 0.007*\"team\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with the TF/IDF model:"
      ],
      "metadata": {
        "id": "apxeLH5JsBRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, score in sorted(lda_model_tfidf[bow_corpus[test_doc]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Topic match score: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, num_of_topics)))"
      ],
      "metadata": {
        "id": "tCCzFiLVr8QD",
        "outputId": "102c4b43-ba42-44c9-c0f3-a3841a3fb016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic match score: 0.9921205043792725\t \n",
            "Topic: 0.004*\"dosa\" + 0.004*\"minist\" + 0.003*\"prime\" + 0.003*\"elect\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Perplexity: ', lda_model.log_perplexity(bow_corpus)) \n",
        "print('Perplexity TFIDF: ', lda_model_tfidf.log_perplexity(bow_corpus)) "
      ],
      "metadata": {
        "id": "Eyz4sodNsKln",
        "outputId": "edc2de67-f90c-4484-824e-7276d82dc6db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity:  -6.707541112476166\n",
            "Perplexity TFIDF:  -8.128765982801298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def optimize_topic(processed_docs,max_num_topic):\n",
        "\n",
        "    perplexity_bow = {}\n",
        "    perplexity_tfidf = {}\n",
        "\n",
        "    keep_the_first_n_tokens = 10000\n",
        "    \n",
        "    dictionary = corpora.Dictionary(processed_docs)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "    # initialize a tfidf from our corpus\n",
        "    tfidf = models.TfidfModel(bow_corpus)\n",
        "\n",
        "    # apply it on our corpus \n",
        "    tfidf_corpus = tfidf[bow_corpus]\n",
        "\n",
        "    for num_topic in range(2, max_num_topic):\n",
        "\n",
        "          lda_model = models.LdaMulticore(bow_corpus, \n",
        "                                num_topics=num_topic, \n",
        "                                id2word=dictionary,\n",
        "                                passes=10, \n",
        "                                workers=2)\n",
        "          \n",
        "          lda_model_tfidf = models.LdaMulticore(tfidf_corpus, \n",
        "                                      num_topics=num_topic, \n",
        "                                      id2word=dictionary, \n",
        "                                      passes=10, \n",
        "                                      workers=4)\n",
        "          \n",
        "          perplexity_bow[num_topic] = lda_model.log_perplexity(bow_corpus)\n",
        "\n",
        "          perplexity_tfidf[num_topic] = lda_model_tfidf.log_perplexity(tfidf_corpus)\n",
        "\n",
        "\n",
        "    plt.plot(list(perplexity_bow.keys()),list(perplexity_bow.values()),\"b\",label = \"bow\")\n",
        "    plt.plot(list(perplexity_tfidf.keys()),list(perplexity_tfidf.values()), \"g\", label = \"tfidf\")\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    return 0\n",
        "          "
      ],
      "metadata": {
        "id": "VjinBiP4uH6Q"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimize_topic(processed_docs,max_num_topic = 20)"
      ],
      "metadata": {
        "id": "5_0XnEDmv7Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - inference\n",
        "\n",
        "Now try it on a new document!\n",
        "\n",
        "Go to a news website, such as [orf.at](https://orf.at/) and copy an article of your choice here:"
      ],
      "metadata": {
        "id": "WlrqLTEFsTDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.lemonde.fr/en/opinion/article/2022/12/27/speaking-the-truth-about-immigration_6009281_23.html\n",
        "\n",
        "unseen_document = \"\"\"You can share an article by clicking on the share icons at the top right of it. \n",
        "The total or partial reproduction of an article, without the prior written authorization of Le Monde, is strictly forbidden. \n",
        "For more information, see our Terms and Conditions. \n",
        "For all authorization requests, contact droitsdauteur@lemonde.fr. \n",
        "\n",
        "The French reversal may have only a remote connection to migration issues. In each of the target countries,\n",
        "visa restrictions fueled strong criticism of France. But in a context of rising anti-French sentiment throughout\n",
        "Africa and faced with the risk of emerging countries leaving the Western bloc, Paris was able to give precedence\n",
        "to these fundamental concerns over the math of deportations. Other bilateral issues (Western Sahara, gas, etc.),\n",
        "as well as the difficulty of giving in to Morocco without doing the same for Algeria, may also have played roles.\n",
        "The equation cannot be summed up as a simple trade-off between visas and deportations, because relations between \n",
        "France and its former colonies, marked by the presence of large communities of Maghrebi origin, are a matter of \n",
        "both foreign policy and domestic affairs.\"\"\"\n",
        "\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "print(\"Simply printing the lda_model output would look like this:\")\n",
        "pprint(lda_model[bow_vector])\n",
        "\n",
        "print(\"\\n\\nSo let's make it nicer, by printing the topic contents:\")\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
      ],
      "metadata": {
        "id": "jgZV3-Z7sXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "dbF83i4lsena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyLDAvis"
      ],
      "metadata": {
        "id": "seo3yGz0YPUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "bow_lda_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
        "\n",
        "pyLDAvis.display(bow_lda_data)"
      ],
      "metadata": {
        "id": "3DAlOWCwsfBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STlpdQYsYNfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}